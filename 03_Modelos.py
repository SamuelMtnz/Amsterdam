#--------------------------------------------
#               MODELIZACI√ìN
#--------------------------------------------

#Importamos librer√≠as
import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, MinMaxScaler
from scipy.stats import skew

#Librer√≠as gr√°ficos
import seaborn as sns 
import matplotlib.pyplot as plt

#Librer√≠as modelos
from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from xgboost import XGBRegressor
from sklearn.svm import SVR

import joblib
from scipy.stats import randint

# Cross Validation y Pipelines
from sklearn.model_selection import cross_val_score, KFold
from sklearn.pipeline import make_pipeline


# Cargar DB
import pandas as pd
df_house_final = pd.read_csv('datos_procesados.csv')



X = df_house_final[['Area', 'Room', 'Room_Area', 'Lon', 'Lat']]
y = df_house_final['Price']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, shuffle = True)

print(f"üìä Tama√±o de los datos:")
print(f"- üßë‚Äçüéì Entrenamiento: {X_train.shape[0]} filas")
print(f"- üßë‚Äçüè´ Prueba: {X_test.shape[0]} filas")


modelos = {
    'Regrasi√≥n Lineal' : make_pipeline(StandardScaler(), LinearRegression()),
    'Random Forest' :  RandomForestRegressor(random_state = 42),
    'XGBoost' : make_pipeline(StandardScaler(),XGBRegressor(random_state = 42)),
    'SVR' : SVR(),
    'Ridge' : make_pipeline(StandardScaler(), Ridge())
}

resultados = {}

for nombre ,modelo in modelos.items():
    # Entrenamiento
    modelo.fit(X_train, y_train)
    
    # Predicci√≥n
    y_pred = modelo.predict(X_test)
    
    # M√°tricas
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    
    # Cross validation
    cv_scores = cross_val_score(modelo, X_train, y_train, cv=5, scoring='r2')
    avg_r2_cv = np.mean(cv_scores)    
    
    resultados[nombre] = {"MSE": mse, "R¬≤": r2, "MAE": mae, 'R¬≤ CV': avg_r2_cv}

print("\nüìà Resultados de los modelos:")
for modelo, metrics in resultados.items():
    print(f"\nüîπ {modelo}:")
    print(f"- MSE: {metrics['MSE']:.2f}")
    print(f"- R¬≤: {metrics['R¬≤']:.2f}")
    print(f"- MAE: {metrics['MAE']:.2f}")
    print(f"- R¬≤ CV: {metrics['R¬≤ CV']:.2f}")
    
mejor_modelo = max(resultados, key=lambda x: resultados[x]['R¬≤'])
print(f"\nüèÜ Mejor modelo: {mejor_modelo} (R¬≤ = {resultados[mejor_modelo]['R¬≤']:.3f})")


# --------------------------------------------
# Optimizar hiperpar√°metros GridSearch
# --------------------------------------------
param_grid = {
    "n_estimators": [100, 200, 300, 500],
    "max_depth": [None, 10, 20, 30],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4],
    "max_features": ['sqrt', 'log2']
}

rf = RandomForestRegressor(random_state=42)
grid_search = GridSearchCV(
    estimator=rf, 
    param_grid=param_grid,
    cv=5,                     # 5-fold cross validation
    scoring='r2',             # Optimizar para R¬≤
    verbose=1,                # Mostrar progreso
    n_jobs=-1                 # Usar todos los n√∫cleos del CPU
)

grid_search.fit(X_train, y_train)

# Mejores par√°metros
print(f"\nüéØ Mejores par√°metros: {grid_search.best_params_}")

# Evaluar modelo optimizado
mejor_rf = grid_search.best_estimator_
y_pred_rf = mejor_rf.predict(X_test)
print(f"R¬≤ optimizado: {r2_score(y_test, y_pred_rf):.2f}")

# Analizar Residuos
residuals = y_test - y_pred_rf
plt.scatter(y_pred, residuals)
plt.axhline(y=0, color='r', linestyle='-')
plt.title("An√°lisis de residuos")
plt.xlabel("Predicciones")
plt.ylabel("Residuos")
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(residuals, kde=True, bins=30)
plt.title("Distribuci√≥n de Residuos")
plt.xlabel("Residuos")
plt.show()



# Despu√©s de la optimizaci√≥n con GridSearch/RandomizedSearch
import joblib
from pathlib import Path

# Crear directorio para modelos
MODELS_DIR = Path("models")
MODELS_DIR.mkdir(exist_ok=True, parents=True)

# 1. Guardar el mejor modelo general
joblib.dump(mejor_rf, MODELS_DIR / "modelo_general.pkl")

# 2. Guardar el escalador global (fit con todos los datos)
# scaler_global = StandardScaler().fit(X)  # Ajustar con TODOS los datos
# joblib.dump(scaler_global, MODELS_DIR / "escalador_global.pkl")

# 3. Guardar informaci√≥n de caracter√≠sticas (importante para Streamlit)
caracteristicas = list(X.columns)
joblib.dump(caracteristicas, MODELS_DIR / "caracteristicas.pkl")

print("‚úÖ Modelos guardados:")
print(f"- Modelo general: models/modelo_general.pkl")
# print(f"- Escalador global: models/escalador_global.pkl")